# Meeting 17-02

###### Done this week
Has only worked with JabRef files
- Collecting the data:
    - Restructured the program folder (manually):
        - Gathered all files with same given concern to the same folder.
    - Reads directory with given label, and recursively appends the files to 
      a CSV table with the with [*File Name*, *Label*, *File Content*]
- Pre-process:
    - Reads the prepared CSV-file
    - Manipulates the File Content column:
        - Simple parsing with NLTK and regex
- Testing.py:
    - Feature extraction
    - Classifies
    - Evaluation
    - Prints out the models

#### Questions
1. On the right path?:
    1. How we collect the data
    2. The title?:
        1. __Title__: Mapping source code to architectural concerns through machine learning.
   3. Right now we are working with the JabRef program, should we combine the 
       data from the other programs to try and create a more general model?

#### Answers
1.3:
    Later on, look into spanning over other systems.
    Other systems can have other concerns, look at 'new system, how does one map?'

#### To Be Done until next week
- How to do the evaluation (focus upon), can play around with the parameters:
    - size of the training, between 5-30%
    - the selection of classifier
    - the different types of input to compare (pre-processing)
- Create preliminary version of background and motivation in dissertation 
- Improve the workflow-environment (e.g. decouple the code, fix flaws)
- Reconfigure the pre-processor according to prior mentions on what to extract
- Start Evaluating Naive Bayes

#### Notes

- manually does a portion, then classifies the rest
- tip: google for java keywords 
- have to look into the detailed number:
    - understand what is precision and recall
        - if precession is low, you get bad recommendation
        - good to focus on precision
        - recall means, you get all the mappings, if bad the tool runs out of rec.
Ex:
- Run to find classifier -> finds 'cats' and 'not cats'
- Precision, how many are 'cats' / 'cats' + 'not cats'
- Recall, 'found cats' / 'total items'

- Look into k-fold cross validation, and look at Cross-Score, provide the dataset  
  and do the split manually and test the data
- When evaluating, look at two variables, and how one affects the other (one is  
  constant)
- don't need to play around with the hyper parameters of the classifier, throw  
  away the worst ones
