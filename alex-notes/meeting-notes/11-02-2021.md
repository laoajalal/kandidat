# Meeting 11-02
----
#### Algorithms
1. Naive Bayes
2. Max Entropy
3. Support-vector machines
4. K-nearest neighbours
5. Decisiion trees
6. Logistic Regression with extensions (one vs all, one vs one)
#### Pre-processing
Decided upon using:
- NLTK for preprocessing
- Scikit-learn for modelling and evaluation, some more tools may show upp:
    - such as Weights & Biases

**Q** Should we have the dependencies in mind? or just find similar files when we map
out the source code?
- Ans: Better to only have textual analysis?, can store import, extract dependencies  
  at class level
#### Relevant information to extract
###### Alt 1
- function names
- variable/object names
- library names
- package names
- remove:
    - syntax-/keywords
    - characters
- Case-sensitive

#### Model evaluation split
- train-test split (70/30, 80/20)
- k-fold cross-validation
- Leave one out cross-validation
- Scikit-learn also has some embedded


#### Notes
__Alt 1__
- case-sensitive, semantics?
- seperate composite words with different meanings
- remove scope
- extract class level:
    - there is textural description in the repository

- Should think about how to do the evaluation:
    - want to eval:
        - how are the classifiers?
        - how good is the recomendations?
        - try out a __random__ 10% of the system for training
        - can use different rates (20%,30%)

__Training data__
Vi har git repot med olika high level conserns (modules), dem riktiga filerna
skall mappas till en av dem. Vi skall skapa en dataset som har en kolummn f√∂r 
varje module.

- Modules:
    - if there is a single file, then you're lucky
    - but if there is a general import, then extract the mapping to build the 
      class labels
    - need for the labels, textural modules
    - can do the work manually or automatically
    - don't spend to much time to get architectural information:
        - can use the xms

#### ToDo until next week
- Prototype of workflow for creating and evaluation models:
